# Run Table Columns Explanation

This document explains all columns in the `run_table.csv` file, their data sources, units, and how they are collected.

## Overview

The run table contains metrics from three main sources:
1. **Locust** - Performance metrics (throughput, latency, failures)
2. **Prometheus** - Resource usage metrics (CPU, memory)
3. **EnergiBridge** - Energy consumption metrics (package, DRAM, PP0, CPU frequency, memory)

---

## Column Descriptions

### Experiment Control Columns

#### `__run_id`
- **Description**: Unique identifier for each run (e.g., `run_0_repetition_0`)
- **Source**: Generated by Experiment Runner
- **Format**: `run_{N}_repetition_{M}` where N is the run number and M is the repetition number

#### `__done`
- **Description**: Status indicator for the run
- **Source**: Experiment Runner
- **Values**: `DONE` (completed), `TODO` (pending), or error status

#### `topology`
- **Description**: muBench topology type being tested
- **Source**: Experiment factor from `RunTableModel`
- **Values** (in this replication data):
  - `sequential_fanout`
  - `parallel_fanout`
  - `chain_with_branching`
  - `hierarchical_tree`
  - `probabilistic_tree`
  - `complex_mesh`

#### `system_size`
- **Description**: Number of services in the topology
- **Source**: Experiment factor from `RunTableModel`
- **Values**: `5`, `10`, or `20`

---

### Locust Performance Metrics

These metrics are collected from Locust's CSV output (`results_stats.csv`) after the load test completes.

#### `throughput_rps`
- **Description**: Requests per second (throughput)
- **Source**: Locust CSV - `Requests/s` column from aggregated row
- **Unit**: Requests per second (RPS)
- **Collection**: Parsed from `{run_dir}/locust/results_stats.csv` after Locust execution

#### `avg_latency_ms`
- **Description**: Average response time
- **Source**: Locust CSV - `Average Response Time` column from aggregated row
- **Unit**: Milliseconds (ms)
- **Collection**: Parsed from `{run_dir}/locust/results_stats.csv` after Locust execution

#### `p95_latency_ms`
- **Description**: 95th percentile response time
- **Source**: Locust CSV - `95%` column from aggregated row
- **Unit**: Milliseconds (ms)
- **Collection**: Parsed from `{run_dir}/locust/results_stats.csv` after Locust execution
- **Note**: Represents the latency below which 95% of requests fall

#### `failure_rate`
- **Description**: Fraction of requests that failed
- **Source**: Calculated from Locust CSV - `Failure Count / Request Count` from aggregated row
- **Unit**: Fraction (0.0 to 1.0)
- **Collection**: Parsed from `{run_dir}/locust/results_stats.csv` after Locust execution

#### `request_count`
- **Description**: Total number of requests made during the test
- **Source**: Locust CSV - `Request Count` column from aggregated row
- **Unit**: Count (integer)
- **Collection**: Parsed from `{run_dir}/locust/results_stats.csv` after Locust execution

---

### Prometheus Resource Metrics

These metrics are collected by querying Prometheus (running on the server) via SSH tunnel after the measurement phase ends.

#### `cpu_usage_avg`
- **Description**: Average CPU usage across all pods in the namespace (during the measurement window).
- **Source**: Prometheus query - `rate(container_cpu_usage_seconds_total{namespace="..."}[5m])`
- **Unit**: **CPU cores** (absolute). Value is the average number of cores utilized (e.g. 1.8 = 1.8 cores). Not a percentage. The experiment Minikube cluster is configured with **28 vCPUs** per node, with 4 reserved for the kubelet, so **24 cores are allocatable** to workloads (`muBench/scripts/setup-infrastructure.sh`). Thus 1.8 cores ≈ 1.8/24 ≈ 7.5% of allocatable CPU.
- **Collection**: Query Prometheus API via SSH tunnel; result written to `{run_dir}/prometheus_cpu.txt`

**Why is CPU utilization so low (~1–2 cores, ~7–8% of allocatable)?** (1) **Throughput is modest** — observed throughput is ~6–17 RPS; total CPU work per second is limited by how many requests complete. (2) **Small CPU work per request** — each request runs the internal “loader” with CPU stress: pi to 100 digits × 20 trials, single-thread (`range_complexity` [100,100], `trials` 20 in the workmodels). So each request burns only a modest amount of CPU. (3) **Request time is mostly non-CPU** — average latency is ~4–13 s; most of that is network, serialization, and dependency calls along the topology, not raw compute. (4) **Load is fixed** — Locust runs with 100 users and 10 min duration; the bottleneck is end-to-end latency and throughput, not saturation of the 24 allocatable cores. So low CPU is expected for this workload and is consistent with a latency- and throughput-bound regime rather than a CPU-bound one.

#### `memory_usage_avg`
- **Description**: Average memory usage across all pods in the namespace
- **Source**: Prometheus query - `container_memory_working_set_bytes{namespace="..."}`
- **Unit**: Bytes
- **Collection**: Query Prometheus API via SSH tunnel; result written to `{run_dir}/prometheus_memory.txt`

---

### EnergiBridge Energy Metrics

These metrics are collected from EnergiBridge, which measures system-wide energy consumption using Intel RAPL (Running Average Power Limit) counters. EnergiBridge runs on the server and outputs CSV files with detailed metrics.

#### `energy`
- **Description**: Total CPU package energy consumption
- **Source**: EnergiBridge - `PACKAGE_ENERGY (J)` column delta (last - first value)
- **Unit**: Joules (J)
- **Collection**: Parsed from EnergiBridge CSV; delta with overflow correction applied

#### `dram_energy`
- **Description**: DRAM (memory) energy consumption
- **Source**: EnergiBridge - `DRAM_ENERGY (J)` column delta (last - first value)
- **Unit**: Joules (J)
- **Collection**: Parsed from EnergiBridge CSV; delta with overflow correction applied

#### `cpu_usage_eb_avg`
- **Description**: Average CPU usage across all cores (from EnergiBridge)
- **Source**: EnergiBridge - Average of all `CPU_USAGE_*` columns (one per core)
- **Unit**: Percentage (%)

#### `cpu_freq_avg`
- **Description**: Average CPU frequency across all cores
- **Source**: EnergiBridge - Average of all `CPU_FREQUENCY_*` columns (one per core)
- **Unit**: Megahertz (MHz)

#### `memory_used_avg`
- **Description**: Average used system memory
- **Source**: EnergiBridge - `USED_MEMORY` column average
- **Unit**: Bytes

#### `memory_total`
- **Description**: Total system memory
- **Source**: EnergiBridge - `TOTAL_MEMORY` column (constant value)
- **Unit**: Bytes

---

## Raw Run Folders

Each row in `run_table.csv` corresponds to one folder under `raw_runs/` (e.g. `run_0_repetition_0`). Those folders hold the underlying artifacts:

- **Locust**: `locust/results_stats.csv`, `locust/results_stats_history.csv`, `locust/locust_output.txt`, `locust/results.html`, etc.
- **Prometheus**: `prometheus_cpu.txt`, `prometheus_memory.txt`
- **EnergiBridge**: `energibridge_output.csv`

Column definitions above describe how each `run_table.csv` column is derived from these files.
